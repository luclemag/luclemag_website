@article{7435254,
 abstract = {The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in nonconvex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising and the approximation of large matrices arising in inverse problems.},
 author = {Le Magoarou, Luc and Gribonval, RÃ©mi},
 doi = {10.1109/JSTSP.2016.2543461},
 issn = {1941-0484},
 journal = {IEEE Journal of Selected Topics in Signal Processing},
 keywords = {Sparse matrices;Dictionaries;Complexity theory;Transforms;Inverse problems;Approximation algorithms;Optimization;Sparse representations;fast algorithms;dictionary learning;low complexity;image denoising;inverse problems;Sparse representations;fast algorithms;dictionary learning;low complexity;image denoising;inverse problems},
 month = {June},
 number = {4},
 pages = {688-700},
 title = {Flexible Multilayer Sparse Approximations of Matrices and Applications},
 volume = {10},
 year = {2016}
}

